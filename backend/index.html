<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>AI Voice Assistant</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    body {
      background-color: #000;
      color: white;
      font-family: 'Segoe UI', sans-serif;
      display: flex;
      flex-direction: column;
      justify-content: center;
      align-items: center;
      height: 100vh;
      margin: 0;
      overflow: hidden;
    }

    .orb-container {
      position: relative;
    }

    .orb {
      width: 200px;
      height: 200px;
      background: radial-gradient(circle at 30% 30%, #00ffff, #004d4d);
      border-radius: 50%;
      box-shadow: 0 0 40px #00ffff88;
      cursor: pointer;
      display: flex;
      align-items: center;
      justify-content: center;
      z-index: 2; /* Ensures clickability */
    }

    .ring {
      position: absolute;
      top: -20px;
      left: -20px;
      width: 240px;
      height: 240px;
      border-radius: 50%;
      box-shadow: 0 0 30px #00ffff55;
      animation: pulse 2s infinite;
      z-index: 1;
      pointer-events: none; /* Prevents click interference */
    }
    
    .orb.listening {
      background: radial-gradient(circle at 30% 30%, #00ffcc, #005c5c);
      box-shadow: 0 0 50px #00ffff;
    }

    .orb.playing {
      background: radial-gradient(circle at 30% 30%, #ff00ff, #520052);
      box-shadow: 0 0 50px #ff00ff99;
    }

    .ring.listening {
      animation: vibrate 0.4s infinite;
    }

    .ring.playing {
      animation: glow 1s ease-in-out infinite alternate;
    }

    @keyframes pulse {
      0%, 100% { transform: scale(1); opacity: 0.6; }
      50% { transform: scale(1.2); opacity: 1; }
    }

    @keyframes vibrate {
      0% { transform: scale(1.05); }
      50% { transform: scale(1.15); }
      100% { transform: scale(1.05); }
    }

    @keyframes glow {
      0% { box-shadow: 0 0 40px #ff00ff88; }
      100% { box-shadow: 0 0 60px #ff00ffcc; }
    }

    .status-text {
      margin-top: 20px;
      font-size: 18px;
      color: #aaa;
      font-weight: 300;
      font-style: italic;
    }
  </style>
</head>
<body>
  <div class="orb-container">
    <div id="ring" class="ring"></div>
    <div id="orb" class="orb" title="Click to speak with AI"></div>
  </div>
  <div id="status" class="status-text">Click to speak</div>

  <audio id="beep" src="https://www.soundjay.com/button/sounds/button-3.mp3" preload="auto"></audio>

  <script>
    const orb = document.getElementById('orb');
    const ring = document.getElementById('ring');
    const statusText = document.getElementById('status');
    const beep = document.getElementById('beep');

    let recognition;
    let lastTranscript = "";
    let silenceTimeout;

    // Initialize speech recognition
    function startRecognition() {
      if (!('webkitSpeechRecognition' in window)) {
        alert('This browser does not support speech recognition.');
        return;
      }

      beep.play();
      recognition = new webkitSpeechRecognition();
      recognition.lang = 'en-US'; // Change to your preferred language
      recognition.interimResults = true;
      recognition.continuous = true;
      lastTranscript = "";

      recognition.onstart = () => {
        orb.classList.add('listening');
        ring.classList.add('listening');
        statusText.textContent = "Listening...";
      };

      recognition.onresult = (event) => {
        for (let i = event.resultIndex; i < event.results.length; ++i) {
          if (event.results[i].isFinal) {
            lastTranscript += event.results[i][0].transcript;
          }
        }

        // Reset silence timeout
        clearTimeout(silenceTimeout);
        silenceTimeout = setTimeout(stopRecognition, 1000);
      };

      recognition.onerror = (event) => {
        console.error('Error:', event.error);
        orb.classList.remove('listening');
        ring.classList.remove('listening');
        statusText.textContent = "Error: " + event.error;
      };

      recognition.onend = () => {
        orb.classList.remove('listening');
        ring.classList.remove('listening');
      };

      recognition.start();
    }

    // Stop recognition and send to webhook
    async function stopRecognition() {
      if (recognition) {
        recognition.stop();
        orb.classList.remove('listening');
        ring.classList.remove('listening');
        statusText.textContent = "Processing...";

        if (lastTranscript.trim() !== '') {
          try {
            console.log("Sending to webhook:", lastTranscript.trim());
            // IMPORTANT: Replace YOUR_WEBHOOK_URL_HERE with the actual webhook URL from the 'Audio Processing Endpoint' node
            const response = await fetch("http://localhost:5678/webhook/process-audio", {
              method: "POST",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify({ question: lastTranscript.trim() })
            });

            if (!response.ok) throw new Error("Server response error");

            const blob = await response.blob();
            const audioURL = URL.createObjectURL(blob);
            const audio = new Audio(audioURL);

            audio.onplay = () => {
              orb.classList.add('playing');
              ring.classList.add('playing');
              statusText.textContent = "Responding...";
            };

            audio.onended = () => {
              orb.classList.remove('playing');
              ring.classList.remove('playing');
              statusText.textContent = "Click to speak";
            };

            audio.play();

          } catch (err) {
            console.error("Error sending or processing response:", err);
            statusText.textContent = "Error communicating with AI";
          }
        }
      }
    }

    // Click handler for the orb
    orb.addEventListener('click', () => {
      if (recognition && recognition.running) {
        stopRecognition();
      } else {
        startRecognition();
      }
    });
  </script>
</body>
</html>
